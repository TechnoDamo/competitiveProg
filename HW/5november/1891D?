Я проверил объяснение решения, и я не могу понять один ключевой элемент. 
В объяснении говорится, что на каждом отрезке с f(i)==f(i+1) у нас будет не более O(log n) переходов, таких что g(i)!=g(i+1).
Мой вопрос: как может g(i)!=g(i+1) встречаться до n раз на отрезке, где f(i)==f(i+1), когда f(x)=log2(x), в то время как g(x) = logf(x)x, 
что означает, что основание логарифма g(x) > 2 => такой логарифм должен изменять свои значения значительно реже, чем log2(x).
